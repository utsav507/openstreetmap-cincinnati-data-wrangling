{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenStreetMap - Cincinnati - Data Wrangling Project\n",
    "\n",
    "OpenStreetMap (OSM) is a collaborative project to create a free editable map of the world.Map data is collected from scratch by volunteers performing systematic ground surveys using tools such as a handheld GPS unit, a notebook, digital camera, or a voice recorder. The data is then entered into the OpenStreetMap database.\n",
    "\n",
    "This data can be obtained in two formats: Extensible Markup Language (XML) and Protocol Buffer Binary Format (PBF). I am going to use XML format for this project.\n",
    "\n",
    "The map area that I will be working on this project is the city of Cincinnati, Ohio. The data was obtained using 'MapZen - Metro Extracts'. The size of the data is approximately 198 MB.\n",
    "\n",
    "Since this data is user generated and multiple users can contribute simultaneously, there is a possibility that, the data might have some inconsistencies. The aim of this project is to apply data wrangling techniques to:\n",
    "1. Audit the dataset to find any inconsistencies\n",
    "2. Clean the dataset and write the cleaned data to a file\n",
    "3. Use the files created to load the data into a SQL Server Database\n",
    "4. Extract insights from the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auditing the Data\n",
    "\n",
    "I perform this step to find any inconsistencies in the data, so that I can address them and bring represent them in a consistent format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first start out finding the total number of users that have contributed to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users:  531\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "def get_user_id(element):\n",
    "    return element.attrib[\"uid\"]\n",
    "\n",
    "def get_users(filename):\n",
    "    users = set()\n",
    "    \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == \"node\" or element.tag == \"way\" or element.tag == \"relation\":\n",
    "            user = get_user_id(element)\n",
    "            users.add(user)\n",
    "    return users\n",
    "\n",
    "filename = 'cincinnati.osm'\n",
    "users = get_users(filename)\n",
    "\n",
    "print('Total number of users: ', len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then try to find street names that are incosistent or different from generic street names. Inconsistent street names can include names written with abbreviations or names that have special characters in them or other generic street names that are missing from my expected list of street names. The code below helps me find such street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'ter': {'firshade ter'}, 'Rd': {'Bridgetown Rd', 'Salem Rd'}, 'avenue': {'Hamilton avenue'}, 'Highway': {'Dixie Highway'}, 'St': {'Chickasaw St', 'Clark St', 'E 7th St', 'W Fifth St', 'Greenup St', '215 Calhoun St', 'Vine St'}, 'Ln': {'Fox Cub Ln'}, 'Ave': {'Kenard Ave', 'Erie Ave', 'Whetsel Ave', 'W Clifton Ave', 'Harrison Ave'}, 'Pkwy': {'Valley Plaza Pkwy'}, 'Plaza': {'Procter & Gamble Plaza'}, 'Circle': {'Mount Adams Circle'}, 'Hill': {'Liberty Hill'}, 'State': {'Ritchie State'}, 'Warner': {'Warner'}, 'Rd.': {'Montgomery Rd.'}, 'Crossing': {'North Bend Crossing'}, '508': {'508'}, 'Avnue': {'South Fort Thomas Avnue'}, 'Ludlow': {'Ludlow'}, 'Pass': {'Bremen Pass'}})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected_street_names = ['Street', 'Avenue', 'Boulevard', 'Drive', 'Court', 'Place', 'Square', 'Lane', 'Road', 'Trail', 'Parkway', 'Commons', 'Way', 'Pike']\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_street_names:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == 'addr:street')\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, 'r', encoding='utf8')\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events = ('start',)):\n",
    "        if elem.tag == 'node' or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "street_types = audit(filename)\n",
    "\n",
    "print(street_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from the above code:\n",
    "\n",
    "defaultdict(<class 'set'>, {'Pike': {'Wooster Pike', 'Highland Pike', 'Springfield Pike', 'Cleves Warsaw Pike', 'Alexandria Pike', 'Buttermilk Pike'}, 'ter': {'firshade ter'}, 'Rd': {'Bridgetown Rd', 'Salem Rd'}, 'Way': {'Albert Sabin Way', 'Campus Way', 'Levee Way', 'Distillery Way', 'Aquarium Way', 'East Mehring Way', 'Clock Tower Way', 'West Pete Rose Way'}, 'avenue': {'Hamilton avenue'}, 'Highway': {'Dixie Highway'}, 'St': {'E 7th St', 'Clark St', '215 Calhoun St', 'Greenup St', 'W Fifth St', 'Vine St', 'Chickasaw St'}, 'Ln': {'Fox Cub Ln'}, 'Ave': {'W Clifton Ave', 'Harrison Ave', 'Kenard Ave', 'Whetsel Ave', 'Erie Ave'}, 'Pkwy': {'Valley Plaza Pkwy'}, 'Plaza': {'Procter & Gamble Plaza'}, 'Circle': {'Mount Adams Circle'}, 'Hill': {'Liberty Hill'}, 'State': {'Ritchie State'}, 'Warner': {'Warner'}, 'Rd.': {'Montgomery Rd.'}, 'Crossing': {'North Bend Crossing'}, '508': {'508'}, 'Avnue': {'South Fort Thomas Avnue'}, 'Ludlow': {'Ludlow'}, 'Pass': {'Bremen Pass'}})\n",
    "\n",
    "Based on this output, I created a mapping of inconsistent street names and used it to update street names while inserting them in a csv file. The mapping and the code to update the street names can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firshade ter => firshade Terrace\n",
      "Bridgetown Rd => Bridgetown Road\n",
      "Salem Rd => Salem Road\n",
      "Hamilton avenue => Hamilton Avenue\n",
      "Dixie Highway => Dixie Highway\n",
      "Chickasaw St => Chickasaw Street\n",
      "Clark St => Clark Street\n",
      "E 7th St => E 7th Street\n",
      "W Fifth St => W Fifth Street\n",
      "Greenup St => Greenup Street\n",
      "215 Calhoun St => 215 Calhoun Street\n",
      "Vine St => Vine Street\n",
      "Fox Cub Ln => Fox Cub Lane\n",
      "Kenard Ave => Kenard Avenue\n",
      "Erie Ave => Erie Avenue\n",
      "Whetsel Ave => Whetsel Avenue\n",
      "W Clifton Ave => W Clifton Avenue\n",
      "Harrison Ave => Harrison Avenue\n",
      "Valley Plaza Pkwy => Valley Plaza Parkway\n",
      "Procter & Gamble Plaza => Procter & Gamble Plaza\n",
      "Mount Adams Circle => Mount Adams Circle\n",
      "Liberty Hill => Liberty Hill\n",
      "Ritchie State => Ritchie State\n",
      "Warner => Warner\n",
      "Montgomery Rd. => Montgomery Road\n",
      "North Bend Crossing => North Bend Crossing\n",
      "508 => 508\n",
      "South Fort Thomas Avnue => South Fort Thomas Avenue\n",
      "Ludlow => Ludlow\n",
      "Bremen Pass => Bremen Pass\n"
     ]
    }
   ],
   "source": [
    "mapping = {\n",
    "    \"St\": \"Street\",\n",
    "    \"St.\": \"Street\",\n",
    "    \"Rd\": \"Road\",\n",
    "    \"Rd.\": \"Road\",\n",
    "    \"Ave\": \"Avenue\",\n",
    "    \"Avnue\": \"Avenue\",\n",
    "    \"avenue\": \"Avenue\",\n",
    "    \"Pkwy\": \"Parkway\",\n",
    "    \"ter\": \"Terrace\",\n",
    "    \"Ln\": \"Lane\"\n",
    "    }\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m is None:\n",
    "        return name\n",
    "    if m.group() in mapping:\n",
    "        name = name.replace(m.group(), mapping[m.group()])\n",
    "        return name\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "for street_type, ways in street_types.items():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        print (name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output demonstrates how the code would update some street names using the mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the dataset and write the cleaned data to a file\n",
    "\n",
    "Now, I move on to the step of actually writing our data to a csv file. To do this, I first design a schema for our data. This is structure in which the data would be stored in the csv file and this is how it would be imported to a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I iteratively parse the XML file to extract the necessary data, update it and write it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema\n",
    "import pprint\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def shape_element(element, node_attr_fields = NODE_FIELDS, way_attr_fields = WAY_FIELDS,\n",
    "                  problem_chars = problemchars, default_tag_type = 'regular'):\n",
    "    \n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for field in NODE_FIELDS:\n",
    "            node_attribs[field] = element.attrib[field]\n",
    "      \n",
    "        for tag in element.iter('tag'):\n",
    "            \n",
    "            tag_dict = {}\n",
    "            \n",
    "            # Updating the street name using the above declared mapping function if the tag is a street name\n",
    "            if is_street_name(tag):\n",
    "                tag_dict['value'] = update_name(tag.attrib['v'], mapping)\n",
    "            else:\n",
    "                tag_dict['value'] = tag.attrib['v']\n",
    "            \n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if problemchars.match(tag.attrib['k']):\n",
    "                pass\n",
    "            elif ':' in tag.attrib['k']:\n",
    "                tag_dict['type'] = tag.attrib['k'].split(':')[0]\n",
    "                tag_dict['key'] = tag.attrib['k'].split(':', 1)[1]\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = tag.attrib['k']\n",
    "\n",
    "            tags.append(tag_dict)\n",
    "\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "        for field in WAY_FIELDS:\n",
    "            way_attribs[field] = element.attrib[field]\n",
    "\n",
    "        count = 0\n",
    "        \n",
    "        for nd in element.iter('nd'):\n",
    "            nd_dict = {}\n",
    "            nd_dict['id'] = element.attrib['id']\n",
    "            nd_dict['node_id'] = nd.attrib['ref']\n",
    "            nd_dict['position'] = count\n",
    "            count += 1\n",
    "            way_nodes.append(nd_dict)\n",
    "\n",
    "        for tag in element.iter('tag'):\n",
    "            tag_dict = {}\n",
    "            # Updating the street name using the above declared mapping function if the tag is a street name\n",
    "            if is_street_name(tag):\n",
    "                tag_dict['value'] = update_name(tag.attrib['v'], mapping)\n",
    "            else:\n",
    "                tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "\n",
    "            if problemchars.match(tag.attrib['k']):\n",
    "                pass\n",
    "            elif ':' in tag.attrib['k']:\n",
    "                tag_dict['type'] = tag.attrib['k'].split(':')[0]\n",
    "                tag_dict['key'] = tag.attrib['k'].split(':', 1)[1]\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = tag.attrib['k']\n",
    "\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def validate_element(element, validator, schema=db_schema):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    # print(element)\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        for field, errors in validator.errors.items():\n",
    "            message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "            error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: v for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w', encoding = 'utf8') as nodes_file, \\\n",
    "    codecs.open(NODE_TAGS_PATH, 'w', encoding = 'utf8') as nodes_tags_file, \\\n",
    "    codecs.open(WAYS_PATH, 'w', encoding = 'utf8') as ways_file, \\\n",
    "    codecs.open(WAY_NODES_PATH, 'w', encoding = 'utf8') as way_nodes_file, \\\n",
    "    codecs.open(WAY_TAGS_PATH, 'w', encoding = 'utf8') as way_tags_file:\n",
    "        \n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "process_map(filename, validate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These are the files obtained and their respective sizes, after the above process completes:\n",
    "\n",
    "1. nodes.csv         - 74,981 KB\n",
    "2. nodes_tags.csv    - 1,456 KB\n",
    "3. ways.csv          - 8,450 KB\n",
    "4. ways_nodes.csv    - 26,727 KB\n",
    "5. ways_tags.csv     - 10,742 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use the files created to load the data into a SQL Server Database\n",
    "\n",
    "For the next phase, I will be using SQL Server to create a database of my extracted data. I start off by individually importing the csv files into the database using SQL Server Import Export Wizard.\n",
    "\n",
    "While importing each file I describe the table schema of the data being imported. Here are the table creation queries that I used during import:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Table: Nodes\n",
    "   Query:\n",
    "   CREATE TABLE nodes (\n",
    "       id INT NOT NULL,\n",
    "       lat FLOAT NOT NULL,\n",
    "       lon FLOAT NOT NULL,\n",
    "       user VARCHAR(100) NOT NULL,\n",
    "       uid INT NOT NULL,\n",
    "       version INT NOT NULL,\n",
    "       changeset INT NOT NULL,\n",
    "       timestamp VARCHAR(50) NOT NULL,\n",
    "       PRIMARY KEY(id)\n",
    "   );\n",
    "   \n",
    "2. Table: Nodes_Tags\n",
    "   Query:\n",
    "   CREATE TABLE nodes_tags (\n",
    "       id INT NOT NULL,\n",
    "       key VARCHAR(100) NOT NULL,\n",
    "       value VARCHAR(1000) NOT NULL,\n",
    "       type VARCHAR(100) NOT NULL,\n",
    "       FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\n",
    "3. Table: Ways\n",
    "   Query:\n",
    "   CREATE TABLE ways (\n",
    "       id INT NOT NULL,\n",
    "       user varchar(100) NOT NULL,\n",
    "       uid INT NOT NULL,\n",
    "       version INT NOT NULL,\n",
    "       changeset INT NOT NULL,\n",
    "       timestamp VARCHAR(50) NOT NULL,\n",
    "       PRIMARY KEY(id)\n",
    "   );\n",
    "   \n",
    "4. Table: Ways_Tags\n",
    "   Query:\n",
    "   CREATE TABLE ways_tags (\n",
    "       id INT NOT NULL,\n",
    "       key VARCHAR(100) NOT NULL,\n",
    "       value VARCHAR(1000) NOT NULL,\n",
    "       type VARCHAR(100) NOT NULL,\n",
    "       FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "   \n",
    "5. Table: Ways_Nodes\n",
    "   Query:\n",
    "   CREATE TABLE ways_nodes (\n",
    "       id INT NOT NULL,\n",
    "       node_id INT NOT NULL,\n",
    "       position INT NOT NULL,\n",
    "       FOREIGN KEY (id) REFERENCES ways(id),\n",
    "       FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Extract insights from the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Total number of unique users that have contributed to the data:\n",
    "\n",
    "SELECT COUNT(distinct c.uid) AS 'User Count'\n",
    "<br>\n",
    "FROM (SELECT a.uid FROM nodes a UNION ALL SELECT b.uid FROM ways b) c;\n",
    "<br>\n",
    "<br>\n",
    "User Count\n",
    "<br>\n",
    "510"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Total number of nodes:\n",
    "\n",
    "SELECT COUNT(DISTINCT id) AS 'Nodes Count' FROM nodes;\n",
    "<br>\n",
    "<br>\n",
    "Nodes Count\n",
    "<br>\n",
    "911557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. total number of ways:\n",
    "\n",
    "SELECT COUNT(DISTINCT id) AS 'Ways Count' FROM ways;\n",
    "<br>\n",
    "<br>\n",
    "Ways Count\n",
    "<br>\n",
    "144078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Top 10 contributing users and their respective contributions:\n",
    "\n",
    "SELECT TOP 10 user_name AS 'User', COUNT(id) AS Contributions\n",
    "<br>\n",
    "FROM nodes\n",
    "<br>\n",
    "GROUP BY user_name\n",
    "<br>\n",
    "ORDER BY contributions DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {float: left;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|User\t|Contributions|\n",
    "|:-----:|:-----------:|\n",
    "|Minh Nguyen\t|355011|\n",
    "|lrhill\t|191508|\n",
    "|Nate_Wessel\t|161953|\n",
    "|woodpeck_fixbot\t|106665|\n",
    "|errorcode\t|12773|\n",
    "|reportingsjr\t|12260|\n",
    "|Matt Currie\t|10153|\n",
    "|MichaelGSmith\t|9138|\n",
    "|gmensch\t|7384|\n",
    "|lightbulbsrwarm\t|5803|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. Finding the religions that have places of worship in the city\n",
    "\n",
    "SELECT DISTINCT value AS 'Religion', COUNT(*) AS 'Number of Places of Worship'\n",
    "<br>\n",
    "FROM nodes_tags\n",
    "<br>\n",
    "WHERE id IN (\n",
    "<br>\n",
    "\tSELECT DISTINCT id\n",
    "    <br>\n",
    "\tFROM nodes_tags\n",
    "    <br>\n",
    "\tWHERE value = 'place_of_worship'\n",
    "    <br>\n",
    "\t) AND tag_key = 'religion' GROUP BY value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {float: left;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Religion|Number of Places of Worship|\n",
    "|:----:|:------:|\n",
    "|christian|\t318|\n",
    "|jewish|\t4|\n",
    "|muslim|\t1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see from the above data, majority of the places of worship are Christian and only a handful of other places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f. Finding the 10 most referenced nodes in ways\n",
    "\n",
    "SELECT TOP 10 a.value AS Value, a.tag_key AS 'Tag Key', COUNT(b.node_id) AS Count\n",
    "<br>\n",
    "FROM nodes_tags a, ways_nodes b\n",
    "<br>\n",
    "WHERE a.id = b.node_id \n",
    "<br>\n",
    "GROUP BY a.value, a.tag_key\n",
    "<br>\n",
    "ORDER BY Count DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {float: left;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Value\t|Tag Key|\tCount|\n",
    "|:----:|:----:|:----:|\n",
    "|crossing|\thighway|\t11268|\n",
    "|traffic_signals|\thighway|\t5416|\n",
    "|turning_circle|\thighway|\t4016|\n",
    "|level_crossing|\trailway|\t688|\n",
    "|stop|\thighway|\t667|\n",
    "|tower|\tpower|\t617|\n",
    "|zebra|\tcrossing|\t498|\n",
    "|motorway_junction|\thighway|\t418|\n",
    "|gate|\tbarrier|\t338|\n",
    "|yes|\tcrossing|\t143|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other ideas about the dataset\n",
    "\n",
    "The data is somewhat incomplete to get a wholistic idea of the city. A good improvement to the data collection process would be setting up a set of standards to follow while adding new nodes. The data can be more descreptive wherein the user can add more information about the node to the OSM data. One simple improvement could be adding the start and close date for a node.\n",
    "\n",
    "For exeample, a node could have had a restaurant at that place from the year 2010 to 2014. Later, the restaurant closed and the spot was taken up by a Courier service like Fedex from 2015. In such cases, we might have two different nodes that point to a same location on the map, which can lead to ambiguity. If we add a start date and close date column, we can different between the two establishments that occupied the spot by the dates. So a query to access the node could return two values where a combined primary key of node and start date will allow us differentiate the establishments.\n",
    "\n",
    "Benefits of the Change:\n",
    "On a longer timeline, this change will allow us to identify how a particular city has changed over time. For example, the birth of trains saw the development of a train station in the city around which other businesses developed.\n",
    "\n",
    "Anticipated Problems in Implementing the Change:\n",
    "A user who is adding the node will need to have some prior knowledge about it. In the current scenario, any user can submit information about a location and he/she need not worry about timeline specific details. For example, a tourist visiting Cincinnati for a week can submit info about a location. But if we add timeline based details, he won't be able to submit this data. One way to tackle this problem is to keep the start_date and end_date columns optional, which will allow everyone to submit their inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://docs.python.org/3.3/\n",
    "2. https://docs.microsoft.com/en-us/sql/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
